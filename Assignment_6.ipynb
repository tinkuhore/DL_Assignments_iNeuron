{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e8438d",
   "metadata": {},
   "source": [
    "# 1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "Ans: Convolutional Neural Networks (CNNs) have several advantages over fully connected Deep Neural Networks (DNNs) for image classification:\n",
    "\n",
    "Parameter sharing: In a CNN, the same filters are applied to every part of the image. This allows for parameter sharing, which greatly reduces the number of parameters compared to a fully connected DNN. This makes CNNs more efficient and easier to train.\n",
    "\n",
    "Local receptive fields: CNNs use a local receptive field, which means that each neuron is only connected to a small region of the input. This allows CNNs to capture local features of the image, such as edges and textures, and combine them to form higher-level features.\n",
    "\n",
    "Translation invariance: CNNs are able to recognize the same feature regardless of its location in the image. This is achieved through pooling layers, which downsample the feature maps and ensure that they are invariant to small translations.\n",
    "\n",
    "Hierarchical representation: CNNs are able to learn a hierarchy of features, from low-level features such as edges and textures, to high-level features such as object parts and object categories. This allows CNNs to capture the complex structure of natural images.\n",
    "\n",
    "Overall, the combination of parameter sharing, local receptive fields, translation invariance, and hierarchical representation makes CNNs well-suited for image classification tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303c7bb",
   "metadata": {},
   "source": [
    "# 2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and &quot;same&quot; padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "\n",
    "Ans: To calculate the number of parameters in the CNN, we need to count the number of parameters in each layer and add them up. For each convolutional layer, the number of parameters is given by:\n",
    "\n",
    "(number of input feature maps) * (kernel width) * (kernel height) * (number of output feature maps) + (number of output feature maps)\n",
    "\n",
    "For the first layer, the number of input feature maps is 3 (for RGB), the kernel width and height are both 3, and the number of output feature maps is 100. Therefore, the number of parameters for the first layer is:\n",
    "\n",
    "3 * 3 * 3 * 100 + 100 = 2,800\n",
    "\n",
    "For the second layer, the number of input feature maps is 100 (the output feature maps of the first layer), the kernel width and height are both 3, and the number of output feature maps is 200. Therefore, the number of parameters for the second layer is:\n",
    "\n",
    "100 * 3 * 3 * 200 + 200 = 180,200\n",
    "\n",
    "For the third layer, the number of input feature maps is 200 (the output feature maps of the second layer), the kernel width and height are both 3, and the number of output feature maps is 400. Therefore, the number of parameters for the third layer is:\n",
    "\n",
    "200 * 3 * 3 * 400 + 400 = 1,152,400\n",
    "\n",
    "Adding up the number of parameters for each layer, we get:\n",
    "\n",
    "2,800 + 180,200 + 1,152,400 = 1,335,400\n",
    "\n",
    "Therefore, the total number of parameters in the CNN is 1,335,400.\n",
    "\n",
    "To calculate the amount of RAM required, we need to consider the size of the input images, the number of parameters, and the size of the data type (32-bit floats). When making a prediction for a single instance, the amount of RAM required is:\n",
    "\n",
    "Input images: 200 * 300 * 3 (RGB channels) * 4 (32-bit float) = 720,000 bytes (or 0.72 MB)\n",
    "Parameters: 1,335,400 * 4 (32-bit float) = 5,341,600 bytes (or 5.34 MB)\n",
    "\n",
    "Total RAM required: 0.72 MB + 5.34 MB = 6.06 MB (approximately)\n",
    "\n",
    "When training on a mini-batch of 50 images, the amount of RAM required is:\n",
    "\n",
    "Input images: 50 * 200 * 300 * 3 * 4 = 108,000,000 bytes (or 108 MB)\n",
    "Parameters: 1,335,400 * 4 = 5,341,600 bytes (or 5.34 MB)\n",
    "\n",
    "Total RAM required: 108 MB + 5.34 MB = 113.34 MB (approximately)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1dd3d",
   "metadata": {},
   "source": [
    "# 3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "\n",
    "Ans: Here are five things you could try if your GPU runs out of memory while training a CNN:\n",
    "\n",
    "Reduce the batch size: The batch size is the number of samples processed by the model in each training iteration. If the batch size is too large, it can cause the GPU to run out of memory. Try reducing the batch size to a smaller value, which will reduce the amount of memory required for each iteration.\n",
    "\n",
    "Use mixed precision training: Mixed precision training is a technique that uses lower-precision (e.g., half-precision) floating-point numbers for some computations in the model, which can reduce the amount of memory required for training. This technique requires hardware that supports mixed precision training, but it can significantly reduce memory usage and speed up training.\n",
    "\n",
    "Reduce the size of the model: If the model is too large, it can require a lot of memory to store the parameters and intermediate activations. Try reducing the number of layers or the number of filters in each layer to reduce the size of the model.\n",
    "\n",
    "Use data augmentation: Data augmentation is a technique that generates additional training data by applying random transformations (e.g., rotations, translations, and scaling) to the existing data. This can increase the effective size of the training set and reduce the risk of overfitting. Data augmentation can be performed on the GPU, which can reduce the amount of memory required to store the augmented data.\n",
    "\n",
    "Use a larger GPU: If none of the above approaches work, you may need to upgrade to a larger GPU with more memory. Alternatively, you could consider using a distributed training strategy that allows you to train the model across multiple GPUs or machines, which can increase the amount of memory available for training. However, this approach can be more complex to set up and may require additional resources.\n",
    "\n",
    "\n",
    "# 4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "Ans: Max pooling layers and convolutional layers with the same stride both perform downsampling, meaning they reduce the spatial size of the input feature map. However, there are several reasons why one might prefer to use a max pooling layer rather than a convolutional layer with the same stride:\n",
    "\n",
    "Reduced memory usage: Max pooling layers have fewer parameters than convolutional layers with the same stride, which can reduce the memory usage of the network. This can be especially important for large networks with many layers, where memory constraints can become a limiting factor.\n",
    "\n",
    "Reduced computation: Max pooling layers are faster than convolutional layers with the same stride, since they do not perform any convolutions. This can lead to faster training and inference times.\n",
    "\n",
    "Increased robustness to small variations: Max pooling layers can help to make the network more robust to small variations in the input, since they are less sensitive to exact pixel locations than convolutional layers. This can be particularly useful for tasks such as object recognition, where objects can appear at different locations in the input.\n",
    "\n",
    "Reduced overfitting: Max pooling layers can help to reduce overfitting, since they enforce a degree of spatial invariance by pooling information from nearby pixels. This can prevent the network from overemphasizing small details in the input that may be irrelevant to the task.\n",
    "\n",
    "Of course, there are also situations where a convolutional layer with the same stride might be preferred over a max pooling layer, depending on the specifics of the task and the network architecture. In general, it is important to choose the appropriate downsampling operation based on the specific requirements of the task, the available computational resources, and the desired performance characteristics of the network.\n",
    "\n",
    "\n",
    "# 5. When would you want to add a local response normalization layer?\n",
    "\n",
    "Ans: Local Response Normalization (LRN) layers are a type of normalization layer that is sometimes used in convolutional neural networks (CNNs). LRN layers can be added after a convolutional layer to help normalize the responses across channels at a local scale.\n",
    "\n",
    "There are a few reasons why one might want to add an LRN layer to a CNN:\n",
    "\n",
    "Increased generalization: LRN layers can help to increase the generalization ability of a network, by reducing the sensitivity to the relative scale of different features. This can help prevent the network from overemphasizing certain features in the input and improve its ability to generalize to new examples.\n",
    "\n",
    "Improved robustness: LRN layers can help to improve the robustness of a network to variations in lighting and contrast, by normalizing the responses across channels at a local scale. This can help the network to identify features more reliably, even in challenging conditions.\n",
    "\n",
    "Better performance on certain tasks: LRN layers can be particularly useful for tasks such as object recognition, where the network needs to identify objects at different scales and positions in the input. By normalizing the responses across channels at a local scale, LRN layers can help the network to identify objects more accurately and efficiently.\n",
    "\n",
    "However, it is worth noting that LRN layers are not always necessary or beneficial, and their effectiveness can depend on the specifics of the task and the network architecture. In some cases, other normalization techniques such as Batch Normalization or Group Normalization may be more effective. As with any network architecture decision, it is important to carefully evaluate the benefits and drawbacks of adding an LRN layer and test its performance empirically on the target task.\n",
    "\n",
    "\n",
    "# 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "Ans: Here are the main innovations in each of the networks you mentioned, compared to their predecessors:\n",
    "\n",
    "AlexNet:\n",
    "\n",
    "Use of Rectified Linear Units (ReLU) activation functions, which improved the performance and training speed of the network.\n",
    "Use of Dropout regularization, which helped to prevent overfitting.\n",
    "Use of Local Response Normalization (LRN) layers, which improved the generalization ability of the network.\n",
    "LeNet-5:\n",
    "\n",
    "One of the earliest successful CNN architectures, which popularized the use of convolutional layers and pooling layers for image recognition tasks.\n",
    "GoogLeNet:\n",
    "\n",
    "Introduction of the Inception module, which allowed for more efficient use of computational resources by performing multiple convolution operations in parallel at different scales.\n",
    "Use of global average pooling instead of fully connected layers, which reduced the number of parameters in the network.\n",
    "Integration of auxiliary classifiers at intermediate layers, which helped to improve the gradient flow during training and prevent overfitting.\n",
    "ResNet:\n",
    "\n",
    "Introduction of the residual block, which allowed for deeper networks to be trained by addressing the vanishing gradient problem.\n",
    "Use of skip connections, which allowed for information to be passed directly across layers without being diluted by repeated convolution operations.\n",
    "SENet:\n",
    "\n",
    "Introduction of the Squeeze-and-Excitation (SE) block, which allows the network to adaptively recalibrate the importance of each feature map based on its relevance to the target task.\n",
    "Use of global pooling and channel gating, which reduced the number of parameters in the network and improved its efficiency.\n",
    "Xception:\n",
    "\n",
    "Use of depthwise separable convolutions, which allow for more efficient use of computational resources by separating the spatial and channel-wise convolutions.\n",
    "Introduction of linear projections between separable convolution layers, which allows for the network to learn more complex feature interactions.\n",
    "\n",
    "\n",
    "# 7. What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "\n",
    "Ans: A fully convolutional network (FCN) is a type of neural network architecture that consists entirely of convolutional layers, with no fully connected layers at the end. FCNs are commonly used for tasks such as image segmentation, where the output of the network is a pixel-wise classification of the input image.\n",
    "\n",
    "To convert a dense layer into a convolutional layer, we need to consider the shape of the input and output tensors. Dense layers are typically used at the end of a neural network, and take in a flattened input tensor of shape (batch_size, input_size) and produce an output tensor of shape (batch_size, output_size).\n",
    "\n",
    "To convert a dense layer to a convolutional layer, we need to reshape the input tensor into a 4D tensor that has spatial dimensions. We can do this by adding a new dimension to the tensor to represent the height and width of the input, and setting the depth to 1. This gives us an input tensor of shape (batch_size, height, width, 1).\n",
    "\n",
    "We can then replace the dense layer with a convolutional layer that has a kernel size equal to the size of the flattened input tensor, and output channels equal to the desired output size. We also need to set the stride and padding of the convolutional layer to ensure that the output tensor has the same shape as the output tensor of the dense layer.\n",
    "\n",
    "For example, suppose we have a dense layer with input size 256 and output size 64. We can convert this to a convolutional layer by reshaping the input tensor to shape (batch_size, 16, 16, 1), and replacing the dense layer with a convolutional layer with kernel size 16x16, output channels 64, and appropriate stride and padding to preserve the output shape.\n",
    "\n",
    "\n",
    "# 8. What is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "Ans: The main technical difficulty of semantic segmentation is the need to maintain high spatial resolution throughout the network, while also capturing global context and producing a compact and accurate output.\n",
    "\n",
    "Unlike image classification, where the output is a single label for the entire image, semantic segmentation requires a pixel-wise classification of the image. This means that the output of the network needs to have the same spatial resolution as the input image, which can be computationally expensive and memory-intensive, especially for high-resolution images.\n",
    "\n",
    "At the same time, the network needs to capture global context in order to accurately classify each pixel, as local features may not be sufficient for complex scenes. This requires a balance between local and global information, which can be challenging to achieve.\n",
    "\n",
    "Another difficulty is handling class imbalance, as some classes may be much less frequent than others in the training data. This can result in biased training and inaccurate segmentation results for underrepresented classes. Various techniques, such as class balancing or data augmentation, can be used to address this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0eeea5",
   "metadata": {},
   "source": [
    "# 9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
    "\n",
    "Ans: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e818017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 56s 119ms/step - loss: 0.2489 - accuracy: 0.9234 - val_loss: 0.0559 - val_accuracy: 0.9825\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 57s 122ms/step - loss: 0.0849 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9858\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 55s 117ms/step - loss: 0.0638 - accuracy: 0.9811 - val_loss: 0.0386 - val_accuracy: 0.9872\n",
      "Test loss: 0.038595885038375854\n",
      "Test accuracy: 0.9872000217437744\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape input data to 4D tensor\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=3, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39288676",
   "metadata": {},
   "source": [
    "# 10. Use transfer learning for large image classification, going through these steps:\n",
    "# a. Create a training set containing at least 100 images per class. For example, you could\n",
    "# classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
    "# alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "# b. Split it into a training set, a validation set, and a test set.\n",
    "# c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
    "# optionally add data augmentation.\n",
    "# d. Fine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657b0c8",
   "metadata": {},
   "source": [
    "Ans :\n",
    "\n",
    "Here's an example of how to use transfer learning for large image classification using TensorFlow:\n",
    "\n",
    "a. Create a training set containing at least 100 images per class:\n",
    "\n",
    "In this example, we will use the \"CIFAR-10\" dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. We will use 80% of the data for training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b8781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-datasets\n",
    "!pip uninstall protobuf -y\n",
    "# pip install protobuf==3.12.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149910f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinku/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (/usr/lib/python3/dist-packages/google/protobuf/internal/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load CIFAR-10 dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/__init__.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_tfds_logging\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_metadata \u001b[38;5;28;01mas\u001b[39;00m _call_metadata\n\u001b[1;32m     46\u001b[0m _metadata \u001b[38;5;241m=\u001b[39m _call_metadata\u001b[38;5;241m.\u001b[39mCallMetadata()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/__init__.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# natural than having to import a third party module.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mepath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m community\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamBasedBuilder\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuilderConfig\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/community/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Community dataset API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mock_builtin_to_use_gfile\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mock_huggingface_import\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder_cls_from_module\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mock\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m epath\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_builder\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_info\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m epath\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_info\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_metadata\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decode\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_info.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_imports_lib\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m naming\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m splits \u001b[38;5;28;01mas\u001b[39;00m splits_lib\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature \u001b[38;5;28;01mas\u001b[39;00m feature_lib\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/splits.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m epath\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m naming\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m proto \u001b[38;5;28;01mas\u001b[39;00m proto_lib\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m units\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/proto/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Public API of the proto package.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_info_generated_pb2 \u001b[38;5;28;01mas\u001b[39;00m dataset_info_pb2  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_generated_pb2 \u001b[38;5;28;01mas\u001b[39;00m feature_pb2  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m     21\u001b[0m SplitInfo \u001b[38;5;241m=\u001b[39m dataset_info_pb2\u001b[38;5;241m.\u001b[39mSplitInfo\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# source: dataset_info.proto\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (/usr/lib/python3/dist-packages/google/protobuf/internal/__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset, info = tfds.load('cifar10', split='train[:80%]', with_info=True)\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Number of training images:\", info.splits['train'].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9a213",
   "metadata": {},
   "source": [
    "b. Split it into a training set, a validation set, and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b352de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/validation/test sets\n",
    "train_set = dataset.shuffle(10000).batch(32)\n",
    "valid_set = tfds.load('cifar10', split='train[80%:90%]').batch(32)\n",
    "test_set = tfds.load('cifar10', split='train[90%:]').batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c416b4",
   "metadata": {},
   "source": [
    "c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function for images\n",
    "def preprocess(image, label):\n",
    "    # Convert image to float32\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Normalize pixel values to [-1, 1]\n",
    "    image = (image / 255.0 - 0.5) * 2.0\n",
    "    # Resize image to (224, 224)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing to train/validation/test sets\n",
    "train_set = train_set.map(preprocess).prefetch(1)\n",
    "valid_set = valid_set.map(preprocess).prefetch(1)\n",
    "test_set = test_set.map(preprocess).prefetch(1)\n",
    "\n",
    "# Define data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomZoom(0.1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcb626",
   "metadata": {},
   "source": [
    "d. Fine-tune a pretrained model on this dataset:\n",
    "\n",
    "We will use the \"MobileNetV2\" model as our base model, which was pretrained on the \"ImageNet\" dataset. We will freeze all the layers in the base model except for the last few layers, which we will fine-tune on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 base model\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), include_top=False, weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze all layers in base model except for last few\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define new classification head\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_set,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
